{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c0d517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Conv2D, MaxPooling2D, Flatten, \\\n",
    "    BatchNormalization, Reshape, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "import librosa\n",
    "import librosa.display\n",
    "from keras_tuner import HyperModel\n",
    "from keras_tuner.tuners import Hyperband\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "import gc\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "from scipy.sparse import (spdiags, SparseEfficiencyWarning, csc_matrix,\n",
    "    csr_matrix, isspmatrix, dok_matrix, lil_matrix, bsr_matrix)\n",
    "warnings.simplefilter('ignore',SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6e3fe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quentin\\anaconda3\\envs\\tensorflow_keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9768cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_memory():\n",
    "    gc.collect()\n",
    "    df_file = get_df_files()\n",
    "    df_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a789840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "PATH_FEATURES_FOLDER = './Features/'\n",
    "PATH_MELSPEC_313_128_FOLDER = PATH_FEATURES_FOLDER + 'melspec_313_128/'\n",
    "\n",
    "def get_df_files():\n",
    "    \n",
    "    #Get file paths and labels\n",
    "    path_files = []\n",
    "    labels = []\n",
    "\n",
    "    #Walk through melspectrogram folders\n",
    "    for subdirectory, directory, files in os.walk(PATH_MELSPEC_313_128_FOLDER):\n",
    "\n",
    "        #Get label using directory folder name\n",
    "        label = subdirectory.split('/')[-1]\n",
    "\n",
    "        #Loop through files\n",
    "        for file in files:\n",
    "            path_file = subdirectory + '/' + file\n",
    "            path_files.append(path_file)\n",
    "            labels.append(label)\n",
    "    \n",
    "    #Create list dictionnary for df\n",
    "    list_dict_file = []\n",
    "    \n",
    "    #Feed dictionnary\n",
    "    for filepath, label in zip(path_files, labels):\n",
    "        \n",
    "        #Get filename\n",
    "        filename = filepath.split('/')[-1]\n",
    "        \n",
    "        #Get filename as list of string\n",
    "        splitted_filename = filename.split('_')\n",
    "        \n",
    "        #Append dictionnary to list\n",
    "        list_dict_file.append({\n",
    "            'filepath' : filepath,\n",
    "            'filename' : filename,\n",
    "            'section' : int(splitted_filename[1]),\n",
    "            'domain_env' : splitted_filename[2],\n",
    "            'dir' : splitted_filename[3],\n",
    "            'sound_type' : splitted_filename[4],\n",
    "            'id' : splitted_filename[5],\n",
    "            'suffix' : '_'.join(splitted_filename[6:]).split('.npy')[0],\n",
    "            'label' : label\n",
    "        })\n",
    "\n",
    "    #Get file dataframe\n",
    "    df_files = pd.DataFrame(list_dict_file)\n",
    "    \n",
    "    #Encode label\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_files['label_encoded'] = label_encoder.fit_transform(df_files['label'])\n",
    "    \n",
    "    return df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "557fc6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduced_cnn_machinery_clf(nb_of_end_layer = 6):\n",
    "    \n",
    "    saved_model = Sequential(\n",
    "        [Input(shape = (128, 313, 1), name = \"Input\"),\n",
    "        Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "        Dropout(rate=0.1),\n",
    "        Conv2D(filters=64, kernel_size=5, padding='same', activation='relu'),\n",
    "         Conv2D(filters=32, kernel_size=5, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2), strides=2),\n",
    "        Dropout(rate=0.1),\n",
    "        Conv2D(filters=128, kernel_size=5, padding='same', activation='relu'),\n",
    "        Conv2D(filters=64, kernel_size=5, padding='same', activation='relu'),\n",
    "        Dropout(rate=0.2),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(7, activation='softmax')]\n",
    "    )\n",
    "\n",
    "    #Load the weights\n",
    "    saved_model.load_weights('Models/machinery_clf_best_model.hdf5')\n",
    "    \n",
    "    #Get reduced model\n",
    "    reduced_model = Sequential()\n",
    "    for layer in saved_model.layers[:nb_of_end_layer+1]:\n",
    "        reduced_model.add(layer)\n",
    "        \n",
    "    #Return reduced model \n",
    "    return reduced_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3d89144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLossLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, nb_classes, alpha=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.nb_classes = nb_classes\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"nb_classes\": self.nb_classes,\n",
    "            \"alpha\": self.alpha,\n",
    "        })\n",
    "        return config\n",
    "        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        #Delete later, check first \n",
    "        print(\"Build of center loss layer input_shape : \" + str(input_shape[0][1]))\n",
    "        \n",
    "        self.centers = self.add_weight(name='centers',\n",
    "                                       shape=(self.nb_classes, input_shape[0][1]),\n",
    "                                       initializer='uniform',\n",
    "                                       trainable=False)\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        # x[0] is batchsize*nb_features, x[1] is batchsize*nb_classes onehot, self.centers is nb_classes*nb_features\n",
    "        \n",
    "        #Calculate new differences between last class centers and new feature samples\n",
    "        #Va dans un premier temps calculer la différence delta entre les samples et les centres\n",
    "        #Et dans un second temps sommer pour chaque feature l'ensemble des différences de tout les samples\n",
    "        delta_centers = K.dot(K.transpose(x[1]), (K.dot(x[1], self.centers) - x[0]))  # nb_classes*nb_features\n",
    "        \n",
    "        #Get number of samples for each classes in batch\n",
    "        center_counts = K.sum(K.transpose(x[1]), axis=1, keepdims=True) + 1  # nb_classes*1\n",
    "        \n",
    "        #Get mean of delta centers\n",
    "        #Ici on va enfin moyenner les features de différence de chaque classe\n",
    "        delta_centers /= center_counts\n",
    "        \n",
    "        #Update new centers\n",
    "        new_centers = self.centers - self.alpha * delta_centers\n",
    "        self.add_update((self.centers, new_centers), x)\n",
    "\n",
    "        #Calculate difference between features samples and their class centers\n",
    "        self.result = x[0] - K.dot(x[1], self.centers)\n",
    "        \n",
    "        #For each sample sum all the differences squared\n",
    "        self.result = K.sum(self.result ** 2, axis=1, keepdims=True)\n",
    "        \n",
    "        #Return result\n",
    "        return self.result # batchsize*1 (=pour chaque sample une distance avec le centre)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return K.int_shape(self.result)\n",
    "    \n",
    "\n",
    "\n",
    "def center_loss(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"Loss function translating loss calculted by CenterLoss layer\"\"\"\n",
    "        \n",
    "    #Return the sum of all the losses divided by two\n",
    "    return 0.5 * K.sum(y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48a9cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_section_clf_w_centerloss(type_machinery, lambda_centerloss):\n",
    "    \n",
    "    #Get lambda as scientifice notation (ex: 1e-5)\n",
    "    lambda_scientific_notation = \"{:.0e}\".format(lambda_centerloss)\n",
    "    \n",
    "    #Load model\n",
    "    model = load_model('Models/2022_07_10/section_clf_w_centerloss_' + type_machinery \\\n",
    "                     + '_' + lambda_scientific_notation + '.hdf5', \n",
    "                       custom_objects={\n",
    "                           'CenterLossLayer': CenterLossLayer,\n",
    "                           'center_loss' : center_loss\n",
    "                       }\n",
    "                      )\n",
    "    \n",
    "    #Return model\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9086797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aux_out_model(complete_model):\n",
    "    return Model(complete_model.inputs,  complete_model.get_layer('aux_out').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a5ebf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids_centerloss(complete_model):\n",
    "    \n",
    "    #Get centerloss_layer\n",
    "    centerloss_layer = complete_model.get_layer('centerlosslayer')\n",
    "    \n",
    "    #Return centroids\n",
    "    return centerloss_layer.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c64a3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_embedding_section_soundtype(type_machinery, model_section_clf, directory='train', sound_type='normal',\n",
    "                                        nb_samples_per_class = 500, nb_classes=3):\n",
    "    \n",
    "    #Get type machinery clf\n",
    "    machinery_model = get_reduced_cnn_machinery_clf()\n",
    "    \n",
    "    #Get data from test dataset\n",
    "    df_file = get_df_files()\n",
    "    df_file_sampled = df_file[(df_file['label']==type_machinery) & (df_file['dir']==directory) \\\n",
    "        & (df_file['sound_type']==sound_type)].groupby(['section']).apply(lambda group: group.sample(nb_samples_per_class))\n",
    "    \n",
    "    #Get input data, section and sound_types\n",
    "    filepaths = df_file_sampled['filepath'].tolist()\n",
    "    data = np.asarray(np.asarray([np.load(file).reshape(128, 313, 1) for file in filepaths]))\n",
    "    sections = df_file_sampled['section']\n",
    "    sound_types = df_file_sampled['sound_type']\n",
    "    sections_as_cat_one_hot = tf.keras.utils.to_categorical(sections, num_classes=nb_classes)\n",
    "    \n",
    "    #Get embeddings\n",
    "    embeddings = model_section_clf.predict((machinery_model.predict(data), sections_as_cat_one_hot))\n",
    "    embeddings = embeddings.reshape(nb_samples_per_class*nb_classes, 128)\n",
    "    \n",
    "    return embeddings, sections, sound_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86248f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_embeddings_centroids(embeddings, sections, centers, nb_classes=3):\n",
    "    \n",
    "    #Get one hot encoded sections \n",
    "    sections_as_cat_one_hot = tf.keras.utils.to_categorical(sections, num_classes=nb_classes)\n",
    "    \n",
    "    #embeddings: nb_samples * features\n",
    "    #sections_as_cat_one_hot: nb_samples * nb_classes\n",
    "    #centers: nb_classe * features\n",
    "    \n",
    "    #Calculate difference between centers and embeddings samples: nb_samples * features\n",
    "    differences = embeddings - K.dot(tf.convert_to_tensor(sections_as_cat_one_hot), centers)\n",
    "    \n",
    "    #Calculate distance: nb_samples * 1\n",
    "    distances = K.sum((differences ** 2)**0.5, axis=1, keepdims=True)\n",
    "    \n",
    "    #Return a the distance vector nb_samples * 1\n",
    "    return distances.numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eafbabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances_section_specific(all_distances, sections, section_wanted):\n",
    "    return all_distances[sections == section_wanted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83c8db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_percentile_threshold_foreach_sections(distances, sections, percentile=95):\n",
    "    \n",
    "    #distances: nb_samples * 1\n",
    "    #sections : nb_samples * 1\n",
    "    \n",
    "    #Get distances for each section\n",
    "    distances_section0 = get_distances_section_specific(distances, sections, 0)\n",
    "    distances_section1 = get_distances_section_specific(distances, sections, 1)\n",
    "    distances_section2 = get_distances_section_specific(distances, sections, 2)\n",
    "    \n",
    "    #Get threshold for each section\n",
    "    threshold_section0 = np.percentile(distances_section0, percentile)\n",
    "    threshold_section1 = np.percentile(distances_section1, percentile)\n",
    "    threshold_section2 = np.percentile(distances_section2, percentile)\n",
    "    \n",
    "    #Return thresholds\n",
    "    return threshold_section0, threshold_section1, threshold_section2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73ea00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_anomaly(distances, sections, thresholds):\n",
    "    \n",
    "    #Init list of boolean anomaly (True anomoly, False normal)\n",
    "    is_anomaly = []\n",
    "    \n",
    "    #Loop through distances and sections\n",
    "    idx = 0\n",
    "    for distance, section in zip(distances, sections):\n",
    "        \n",
    "        #Get section specific threshold\n",
    "        current_threshold = thresholds[section]\n",
    "        \n",
    "        #Add boolean determining anomaly\n",
    "        is_anomaly.append(distance > current_threshold)\n",
    "        \n",
    "        idx += 1\n",
    "        \n",
    "    #Transform as np array\n",
    "    is_anomaly = np.asarray(is_anomaly)\n",
    "    \n",
    "    return is_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60068e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_thresholds(type_machinery, reduced_sct_clf_model, centers, percentile_threshold):\n",
    "    \n",
    "    #Get embeddings normal from training dataset\n",
    "    embeddings_normal_train, sections_normal_train, sound_types_normal_train = \\\n",
    "        get_list_embedding_section_soundtype(type_machinery, reduced_sct_clf_model)\n",
    "    \n",
    "    #Get distances\n",
    "    distances_normal_train = get_distance_embeddings_centroids(embeddings_normal_train, sections_normal_train, centers)\n",
    "    \n",
    "    #Get thresholds for each sections\n",
    "    threshold_s0, threshold_s1, threshold_s2 = \\\n",
    "        get_distance_percentile_threshold_foreach_sections(distances_normal_train, sections_normal_train, percentile=percentile_threshold)\n",
    "    \n",
    "    #Get list of thresholds\n",
    "    list_thresholds = [threshold_s0, threshold_s1, threshold_s2]\n",
    "    \n",
    "    #Return list\n",
    "    return list_thresholds\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a55a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_pred(type_machinery, reduced_section_clf_model, centers, list_thresholds, directory, sound_type, \n",
    "                nb_samples_per_class=100):\n",
    "\n",
    "    #Get embeddings, sections and sound_types\n",
    "    embeddings, sections, sound_types = \\\n",
    "        get_list_embedding_section_soundtype(type_machinery, reduced_section_clf_model, directory=directory, \n",
    "                                             sound_type=sound_type, nb_samples_per_class = nb_samples_per_class)\n",
    "    \n",
    "    #Get distances\n",
    "    distances = get_distance_embeddings_centroids(embeddings, sections, centers)\n",
    "    \n",
    "    #Get predictions normal test\n",
    "    preds = pred_anomaly(distances, sections, list_thresholds)\n",
    "    \n",
    "    #Get df of normal test\n",
    "    df = pd.DataFrame({\n",
    "        'n' : np.arange(0, len(preds)),\n",
    "        'section' : sections.values.reshape(len(preds)),\n",
    "        'distance' : distances.reshape(len(preds)),\n",
    "        'real_class' : sound_types.replace(['normal', 'anomaly'], [False, True]).values,\n",
    "        'predicted_class' : preds.reshape(len(preds))\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "873b0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_pred_normal_anomaly_test(type_machinery, lambda_centerloss, percentile_threshold):\n",
    "\n",
    "    refresh_memory()\n",
    "\n",
    "    #Get complete section clf\n",
    "    complete_model = load_model_section_clf_w_centerloss(type_machinery, lambda_centerloss)\n",
    "\n",
    "    #Get reduced model to get embeddings\n",
    "    reduced_model = get_aux_out_model(complete_model)\n",
    "\n",
    "    #Get centroids of each sections\n",
    "    centers = get_centroids_centerloss(complete_model)\n",
    "\n",
    "    #Get list of thresholds\n",
    "    list_thresholds = get_list_thresholds(type_machinery, reduced_model, centers, percentile_threshold)\n",
    "\n",
    "    #Get df of normal test\n",
    "    df_normal_test = get_df_pred(type_machinery, reduced_model, centers, list_thresholds, \n",
    "                                 directory='test', sound_type='normal', nb_samples_per_class=100)\n",
    "\n",
    "    #Get df of anomaly test\n",
    "    df_anomaly_test = get_df_pred(type_machinery, reduced_model, centers, list_thresholds, \n",
    "                                 directory='test', sound_type='anomaly', nb_samples_per_class=100)\n",
    "\n",
    "    #Concat both df\n",
    "    df = pd.concat([df_normal_test, df_anomaly_test], axis=0)\n",
    "    \n",
    "    #return df\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "251ad883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fpr_tpr_roc_curve(df_pred_true):\n",
    "    fpr, tpr, thresholds = roc_curve(df_pred_true['real_class'], df_pred_true['predicted_class'])\n",
    "    return fpr, tpr\n",
    "\n",
    "def get_roc_auc_score(df_pred_true):\n",
    "    return roc_auc_score(df_pred_true['real_class'], df_pred_true['predicted_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "51ddad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, auc, percentile_threshold):\n",
    "    \n",
    "    #Plot roc auc score\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve with percentile_threshold : ' + str(round(percentile_threshold, 2)) + ' auc : ' + str(auc))\n",
    "    \n",
    "#     #Print confusion matrix\n",
    "#     print(pd.crosstab(df['real_class'], df['predicted_class'], rownames=['Classes réelles'],\n",
    "#                colnames=['Classes prédites']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a8bf6",
   "metadata": {},
   "source": [
    "# Valve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8ba5958",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build of center loss layer input_shape : 128\n",
      "WARNING:tensorflow:`add_update` `inputs` kwarg has been deprecated. You no longer need to pass a value to `inputs` as it is being automatically inferred.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1500,64,156,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, percentile_threshold \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(percentile_thresholds):\n\u001b[0;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(percentile_thresholds),idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mget_df_pred_normal_anomaly_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_machinery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_centerloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentile_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     fpr, tpr \u001b[38;5;241m=\u001b[39m get_fpr_tpr_roc_curve(df)\n\u001b[0;32m     11\u001b[0m     auc \u001b[38;5;241m=\u001b[39m get_roc_auc_score(df)\n",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36mget_df_pred_normal_anomaly_test\u001b[1;34m(type_machinery, lambda_centerloss, percentile_threshold)\u001b[0m\n\u001b[0;32m     12\u001b[0m centers \u001b[38;5;241m=\u001b[39m get_centroids_centerloss(complete_model)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Get list of thresholds\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m list_thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mget_list_thresholds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_machinery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduced_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentile_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#Get df of normal test\u001b[39;00m\n\u001b[0;32m     18\u001b[0m df_normal_test \u001b[38;5;241m=\u001b[39m get_df_pred(type_machinery, reduced_model, centers, list_thresholds, \n\u001b[0;32m     19\u001b[0m                              directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, sound_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m, nb_samples_per_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Input \u001b[1;32mIn [57]\u001b[0m, in \u001b[0;36mget_list_thresholds\u001b[1;34m(type_machinery, reduced_sct_clf_model, centers, percentile_threshold)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_list_thresholds\u001b[39m(type_machinery, reduced_sct_clf_model, centers, percentile_threshold):\n\u001b[0;32m      2\u001b[0m     \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#Get embeddings normal from training dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     embeddings_normal_train, sections_normal_train, sound_types_normal_train \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 5\u001b[0m         \u001b[43mget_list_embedding_section_soundtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_machinery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduced_sct_clf_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#Get distances\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     distances_normal_train \u001b[38;5;241m=\u001b[39m get_distance_embeddings_centroids(embeddings_normal_train, sections_normal_train, centers)\n",
      "Input \u001b[1;32mIn [52]\u001b[0m, in \u001b[0;36mget_list_embedding_section_soundtype\u001b[1;34m(type_machinery, model_section_clf, directory, sound_type, nb_samples_per_class, nb_classes)\u001b[0m\n\u001b[0;32m     17\u001b[0m sections_as_cat_one_hot \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mto_categorical(sections, num_classes\u001b[38;5;241m=\u001b[39mnb_classes)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Get embeddings\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model_section_clf\u001b[38;5;241m.\u001b[39mpredict((\u001b[43mmachinery_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, sections_as_cat_one_hot))\n\u001b[0;32m     21\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mreshape(nb_samples_per_class\u001b[38;5;241m*\u001b[39mnb_classes, \u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings, sections, sound_types\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_keras\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow_keras\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7186\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7185\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7186\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1500,64,156,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ConcatV2] name: concat"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "type_machinery = 'valve'\n",
    "lambda_centerlosses = [0.000001, 0.0000000001]\n",
    "percentile_thresholds = [90, 95, 99]\n",
    "\n",
    "for lambda_centerloss in lambda_centerlosses:\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for idx, percentile_threshold in enumerate(percentile_thresholds):\n",
    "        plt.subplot(1,len(percentile_thresholds),idx+1)\n",
    "        df = get_df_pred_normal_anomaly_test(type_machinery, lambda_centerloss, percentile_threshold)\n",
    "        fpr, tpr = get_fpr_tpr_roc_curve(df)\n",
    "        auc = get_roc_auc_score(df)\n",
    "        plot_roc_curve(fpr, tpr, auc, percentile_threshold)\n",
    "    \n",
    "    plt.suptitle(type_machinery + ' : lambda centerloss ' + str(lambda_centerloss))\n",
    "    plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea8303e",
   "metadata": {},
   "source": [
    "# bearing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6cce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_machinery = 'bearing'\n",
    "lambda_centerlosses = [0.00000001, 0.00000000001]\n",
    "percentile_thresholds = [90, 95, 99]\n",
    "\n",
    "for lambda_centerloss in lambda_centerlosses:\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for idx, percentile_threshold in enumerate(percentile_thresholds):\n",
    "        plt.subplot(1,len(percentile_thresholds),idx+1)\n",
    "        df = get_df_pred_normal_anomaly_test(type_machinery, lambda_centerloss, percentile_threshold)\n",
    "        fpr, tpr = get_fpr_tpr_roc_curve(df)\n",
    "        auc = get_roc_auc_score(df)\n",
    "        plot_roc_curve(fpr, tpr, auc, percentile_threshold)\n",
    "    \n",
    "    plt.suptitle(type_machinery + ' : lambda centerloss ' + str(lambda_centerloss))\n",
    "    plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aab1ab",
   "metadata": {},
   "source": [
    "# fan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82077e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_machinery = 'fan'\n",
    "lambda_centerlosses = [0.000001]\n",
    "percentile_thresholds = [90, 95, 99]\n",
    "\n",
    "for lambda_centerloss in lambda_centerlosses:\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for idx, percentile_threshold in enumerate(percentile_thresholds):\n",
    "        plt.subplot(1,len(percentile_thresholds),idx+1)\n",
    "        df = get_df_pred_normal_anomaly_test(type_machinery, lambda_centerloss, percentile_threshold)\n",
    "        fpr, tpr = get_fpr_tpr_roc_curve(df)\n",
    "        auc = get_roc_auc_score(df)\n",
    "        plot_roc_curve(fpr, tpr, auc, percentile_threshold)\n",
    "    \n",
    "    plt.suptitle(type_machinery + ' : lambda centerloss ' + str(lambda_centerloss))\n",
    "    plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3869f799",
   "metadata": {},
   "source": [
    "# gearbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type_machinery = 'gearbox'\n",
    "lambda_centerlosses = [0.000000001, 0.0000000001]\n",
    "percentile_thresholds = [90, 95, 99]\n",
    "\n",
    "for lambda_centerloss in lambda_centerlosses:\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for idx, percentile_threshold in enumerate(percentile_thresholds):\n",
    "        plt.subplot(1,len(percentile_thresholds),idx+1)\n",
    "        df = get_df_pred_normal_anomaly_test(type_machinery, lambda_centerloss, percentile_threshold)\n",
    "        fpr, tpr = get_fpr_tpr_roc_curve(df)\n",
    "        auc = get_roc_auc_score(df)\n",
    "        plot_roc_curve(fpr, tpr, auc, percentile_threshold)\n",
    "    \n",
    "    plt.suptitle(type_machinery + ' : lambda centerloss ' + str(lambda_centerloss))\n",
    "    plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad413d",
   "metadata": {},
   "source": [
    "# slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a03998",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_machinery = 'slider'\n",
    "lambda_centerlosses = [0.000000001, 0.0000000001]\n",
    "percentile_thresholds = [90, 95, 99]\n",
    "\n",
    "for lambda_centerloss in lambda_centerlosses:\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for idx, percentile_threshold in enumerate(percentile_thresholds):\n",
    "        plt.subplot(1,len(percentile_thresholds),idx+1)\n",
    "        df = get_df_pred_normal_anomaly_test(type_machinery, lambda_centerloss, percentile_threshold)\n",
    "        fpr, tpr = get_fpr_tpr_roc_curve(df)\n",
    "        auc = get_roc_auc_score(df)\n",
    "        plot_roc_curve(fpr, tpr, auc, percentile_threshold)\n",
    "    \n",
    "    plt.suptitle(type_machinery + ' : lambda centerloss ' + str(lambda_centerloss))\n",
    "    plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f55fbe",
   "metadata": {},
   "source": [
    "# ToyCar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07247b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_machinery = 'ToyCar'\n",
    "lambda_centerlosses = [0.000000001]\n",
    "percentile_thresholds = [90, 95, 99]\n",
    "\n",
    "for lambda_centerloss in lambda_centerlosses:\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for idx, percentile_threshold in enumerate(percentile_thresholds):\n",
    "        plt.subplot(1,len(percentile_thresholds),idx+1)\n",
    "        df = get_df_pred_normal_anomaly_test(type_machinery, lambda_centerloss, percentile_threshold)\n",
    "        fpr, tpr = get_fpr_tpr_roc_curve(df)\n",
    "        auc = get_roc_auc_score(df)\n",
    "        plot_roc_curve(fpr, tpr, auc, percentile_threshold)\n",
    "    \n",
    "    plt.suptitle(type_machinery + ' : lambda centerloss ' + str(lambda_centerloss))\n",
    "    plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ee8fe",
   "metadata": {},
   "source": [
    "# ToyTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85362b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_machinery = 'ToyTrain'\n",
    "lambda_centerlosses = [0.000000001]\n",
    "percentile_thresholds = [90, 95, 99]\n",
    "\n",
    "for lambda_centerloss in lambda_centerlosses:\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for idx, percentile_threshold in enumerate(percentile_thresholds):\n",
    "        plt.subplot(1,len(percentile_thresholds),idx+1)\n",
    "        df = get_df_pred_normal_anomaly_test(type_machinery, lambda_centerloss, percentile_threshold)\n",
    "        fpr, tpr = get_fpr_tpr_roc_curve(df)\n",
    "        auc = get_roc_auc_score(df)\n",
    "        plot_roc_curve(fpr, tpr, auc, percentile_threshold)\n",
    "    \n",
    "    plt.suptitle(type_machinery + ' : lambda centerloss ' + str(lambda_centerloss))\n",
    "    plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581a470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f5ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
